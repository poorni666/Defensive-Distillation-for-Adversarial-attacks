# Defensive-Distillation-for-Adversarial-attacks

The goal of this project is to introduce adversarial attacks on deep neural networks and report the impacts it has on the classification accuracy. Then , to introduce a defence mechanism to tackle the attack , defensive distillation is utilized here to make the deep neural network more robust aganist this attack and the impact on classification accuracies after the attack has been reported. The attack here used is Fast gradient Sign attack which is a gradient based attack focused on adding small pertubation to the images which causes the model to loose its robustness. The following paper is reimplemented https://arxiv.org/pdf/1511.04508 with experimentation of FSGN attack, the experimental setup of the paper is followed in this project as a base.
